{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = nn.Sequential(nn.Linear(1, 10), nn.Sigmoid(), nn.Linear(10,1, bias=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How To Recover weights of the neural network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=1, out_features=10, bias=True)\n",
       "  (1): Sigmoid()\n",
       "  (2): Linear(in_features=10, out_features=1, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_layer = N[0]\n",
    "second_layer = N[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.1753],\n",
       "        [ 0.2632],\n",
       "        [ 0.3580],\n",
       "        [ 0.4935],\n",
       "        [ 0.7921],\n",
       "        [ 0.5748],\n",
       "        [ 0.7542],\n",
       "        [ 0.5756],\n",
       "        [ 0.1214],\n",
       "        [-0.8317]], requires_grad=True)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_layer.weight  # N[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.2180, -0.0862, -0.8238,  0.0953, -0.9646, -0.7808,  0.7816,  0.2438,\n",
       "        -0.3960, -0.6395], requires_grad=True)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_layer.bias  # N[0].bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.2259, -0.0933, -0.2676,  0.0397, -0.0897,  0.1744,  0.0865, -0.1039,\n",
       "         -0.1030,  0.1847]], requires_grad=True)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_layer.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept of autograd and requires_grad = True in Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems that requires_grad = True, and autograd, are needed for recording the history.\n",
    "\n",
    "Not only this, but to compute some derivatives wrt to them. \n",
    "\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html Complete guide.\n",
    "\n",
    "https://jovian.ai/forum/t/what-is-the-use-of-requires-grad-in-tensors/17718 In short, very useful.\n",
    "\n",
    "https://pytorch.org/docs/stable/notes/autograd.html#:~:text=Setting%20requires_grad&text=Parameter%60%60%2C%20that%20allows%20for,its%20input%20tensors%20require%20grad.\n",
    "Hard to understand.\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.Tensor.requires_grad.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you set requires_grad to True to any tensor, then PyTorch will automatically track and calculate gradients w.r.t. that tensor.\n",
    "During backpropagation you need gradients of the Loss function wrt to the weights. \n",
    "This is done with the .backward() method, so during this operation tensors with requires_grad set to True will be used along with the tensor used to call backward() with to calculate the gradients.\n",
    "\n",
    "### Example1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(2.0, requires_grad = True)\n",
    "\n",
    "z = x ** 3 # z=x^3 ---> Dz/Dx = 3x**2\n",
    "\n",
    "z.backward()  # Computes the gradient \n",
    "# print(x.grad) # this is dz/dx.... You can also try to print it if you comment #z.backward() ---> will return None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2., requires_grad=True)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(12.)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad    # Dz/Dx = 3x^2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that above we used a general function z, but in practice, we use the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.)\n",
      "tensor(1.)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(10.0)\n",
    "w  = torch.tensor(3.0, requires_grad=True)\n",
    "b = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "y = w*x + b  # y is 3*5 + 2 = 17\n",
    "# Notice that in this example and the one above, y is not a python funciton, but a pytorch tensor!!!\n",
    "y.backward()\n",
    "\n",
    "print(w.grad) # This returns the derivative of y w.r.t w which is equal to x=5\n",
    "print(b.grad) # This returns the derivative of y w.r.t b which is  1\n",
    "print(x.grad) # This will return None as we haven't set requires_grad to True for the tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch.org/docs/stable/generated/torch.Tensor.backward.html#:~:text=Computes%20the%20gradient%20of%20current,function%20additionally%20requires%20specifying%20gradient%20.\n",
    "\n",
    "As far as I understood, Pytorch will create a graph. Possibly through the neural network, where we use many arrays/tensors. For such tensors, when we set the requires_grad=True, when with the output of the graph we use the method .backward()  (p.s. notice that .backward() is a method for any tensor! So the output also is a tensor), it will compute all the derivatives wrt to such tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One more note: the output, as pytorch tensor, needs to be scalar! In the examples above no problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How To Define A Loss Function in Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Very general example of loss function\n",
    "def my_loss(output, target):\n",
    "    ''' \n",
    "    output: is a tensor, a pytorch tensor. It is the output of the neural network\n",
    "    target: is a tensor, a pytorch tensor. It is facultative argument of the function, when we\n",
    "               have a supervised model. In ODE we won't use it since it is unsupervised, i.e., we don't know\n",
    "               the result function.'''\n",
    "    loss = torch.mean((output - target)**2)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_1 = tensor([[-0.3416,  1.9846, -0.0480],\n",
      "        [-0.5596, -0.3991,  0.0959]], requires_grad=True)\n",
      "target = tensor([[-0.4779,  2.3243,  0.7750],\n",
      "        [-2.8559,  0.8496, -1.2165]])\n",
      "output= tensor(1.0261, grad_fn=<L1LossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Example of code:\n",
    "\n",
    "mae_loss = torch.nn.L1Loss() # Normal arithmetics loss. If you want to use mean square error: .MSELoss\n",
    "\n",
    "input_1 = torch.randn(2, 3, requires_grad=True)\n",
    "print(\"input_1 =\", input_1)\n",
    "\n",
    "target = torch.randn(2, 3)\n",
    "print(\"target =\", target)\n",
    "\n",
    "output = mae_loss(input_1, target)\n",
    "output.backward()\n",
    "print(\"output=\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "L1Loss()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mae_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1667, -0.1667, -0.1667],\n",
       "        [ 0.1667, -0.1667,  0.1667]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_1.grad # Yes, it is the gradient of mean average error f = (x1+x2+x3+x4+x5+x6/n -constants) ---> df/dxi\n",
    "             # Only the signs are not always good, because in the definition they use absolute values | |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## more on torch.autograd.grad: computing gradients/derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000],\n",
      "        [0.5000],\n",
      "        [1.0000],\n",
      "        [1.5000],\n",
      "        [2.0000]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor(np.linspace(0,2,5)[:,None])\n",
    "x.requires_grad = True\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000],\n",
       "        [0.1250],\n",
       "        [1.0000],\n",
       "        [3.3750],\n",
       "        [8.0000]], grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_x = lambda x: x**3\n",
    "\n",
    "y_x_outputs = y_x(x)\n",
    "y_x_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_der_x = torch.autograd.grad(y_x_outputs, x)   GIVES ERROR, scalar error, so we also use\n",
    "#                                                 grad_outputs = torch.ones_like(x)\n",
    "\n",
    "# https://stackoverflow.com/questions/54754153/autograd-grad-for-tensor-in-pytorch#\n",
    "# for more info see the link above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0000],\n",
       "         [ 0.7500],\n",
       "         [ 3.0000],\n",
       "         [ 6.7500],\n",
       "         [12.0000]]),)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_der_x = torch.autograd.grad(outputs=y_x_outputs, inputs=x, grad_outputs=torch.ones_like(y_x_outputs))\n",
    "\n",
    "# This is very similar to calling output.backward()\n",
    "# But with 2 differences:\n",
    "# 1) First .backward() requires to be applied to a scalar. Therefore we took output = mean(outputs)\n",
    "# 2) We use this and not backward, because this is an intermediate step. The final backward is used\n",
    "#    for the neural network and the weights adjusting. These derivatives in here are only an intermediate step\n",
    "#    , more precisely, in this case, only to compute the Loss Function\n",
    "\n",
    "y_der_x    #is a tuple with one element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000],\n",
       "        [ 0.7500],\n",
       "        [ 3.0000],\n",
       "        [ 6.7500],\n",
       "        [12.0000]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_der_x[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 2, 100)[:, None]\n",
    "\n",
    "input_tensor = torch.Tensor(x)\n",
    "input_tensor.requires_grad = True  # Changing its attribute.\n",
    "\n",
    "N = nn.Sequential(nn.Linear(1, 5), nn.Sigmoid(), nn.Linear(5,1, bias=False))\n",
    "Psi_t = lambda x: 3.2 + x * N(x)  # A function, depending on the neural network \n",
    "\n",
    "outputs = Psi_t(input_tensor) \n",
    "\n",
    "output = torch.mean(outputs)   # Scalar output! It is a pytorch tensor, scalar!\n",
    "                               # Required to make output scalar, because .backward() applies only to a scalar.\n",
    "                               # That is, outputs.backward() gives error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output.backward()  # Let's compute the derivatives of output with respect the tensors with requires_grad=True\n",
    "# print(input_tensor.grad) # Derivatives of output wrt to input_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that if we run again output.backward() gives error. It says to retain_graph=True. What does this mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Psi_t_x = torch.autograd.grad(outputs, input_tensor, grad_outputs=torch.ones_like(outputs),\n",
    "                    create_graph=True)[0] \n",
    "\n",
    "# create_graph (bool, optional) – If True, graph of the derivative will be constructed, \n",
    "# allowing to compute higher order derivative products. Default: False.\n",
    "# Especially we need it when we want to compute second derivatives etc.. etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Psi_t_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theoretically, optimizer is a modification of the classical gradient descent that you know, so that it \n",
    "converges to the minima faster. \n",
    "As we know, we use \n",
    "\n",
    "$$w_{t+1} = w_t - h \\cdot \\frac{\\partial E}{\\partial w} $$\n",
    "\n",
    "This is the classical. Optimizers want that it converges faster. Look at the very quick example\n",
    "https://www.geeksforgeeks.org/intuition-of-adam-optimizer/#:~:text=Adam%20optimizer%20involves%20a%20combination,minima%20in%20a%20faster%20pace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "from a code point of view in pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch.org/docs/stable/optim.html#:~:text=optim-,torch.,easily%20integrated%20in%20the%20future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To use torch.optim you have to construct an optimizer object, that will hold the current state and will update the parameters based on the computed gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General optimizer\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# model.parameters() are the parameters to optimize and to change. It will do so,\n",
    "# lr = learning rate\n",
    "\n",
    "optimizer = torch.optim.LBFGS(N.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer has the method .step(), which optimize with one step/epoch the weights of the model.\n",
    "There are 2 ways to implement this:\n",
    "\n",
    "### Method 1\n",
    "used for certain optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-d4f1b9f69410>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m              \u001b[0;31m# After computing the gradients we apply the optimizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "for input, target in dataset:\n",
    "    optimizer.zero_grad()\n",
    "    output = model(input)\n",
    "    loss = loss_fn(output, target)\n",
    "    loss.backward()              # After computing the gradients we apply the optimizer.\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2\n",
    "used for other optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for input, target in dataset:\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        # output = model(input)\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()         # After computing the gradients we apply the optimizer.\n",
    "        return loss             \n",
    "    optimizer.step(closure)     # Here updates the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the hood: optimizer sees which is the loss function, the one in which we apply .backward(). \n",
    "When we created optimizer we also gave it the N.parameters() of the neural network. \n",
    "These will be modified accordingly: it will compute the derivative of the loss function with respect to \n",
    "these parameters. And these parameters will be changed automatically\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
