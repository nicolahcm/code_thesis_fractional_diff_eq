{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f23e2810",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17699fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = nn.Sequential(nn.Linear(1, 5), nn.Sigmoid(), nn.Linear(5,1, bias=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7046ab4",
   "metadata": {},
   "source": [
    "## How To Recover weights of the neural network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf89250d",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_layer = N[0]\n",
    "second_layer = N[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a4e49ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.5758],\n",
       "        [-0.8870],\n",
       "        [ 0.6140],\n",
       "        [-0.9231],\n",
       "        [ 0.7567]], requires_grad=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_layer.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3c61465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 0.1122, -0.3966, -0.6876, -0.2249, -0.1588], requires_grad=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_layer.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64140740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.2380, -0.3489,  0.3994, -0.3690, -0.1881]], requires_grad=True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_layer.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394417e8",
   "metadata": {},
   "source": [
    "## Concept of autograd and requires_grad = True in Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd861849",
   "metadata": {},
   "source": [
    "Seems that requires_grad = True, and autograd, are needed for recording the history.\n",
    "\n",
    "Not only this, but to compute some derivatives wrt to them. \n",
    "\n",
    "https://jovian.ai/forum/t/what-is-the-use-of-requires-grad-in-tensors/17718\n",
    "\n",
    "https://pytorch.org/docs/stable/notes/autograd.html#:~:text=Setting%20requires_grad&text=Parameter%60%60%2C%20that%20allows%20for,its%20input%20tensors%20require%20grad.\n",
    "Hard to understand.\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.Tensor.requires_grad.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e999569a",
   "metadata": {},
   "source": [
    "If you set requires_grad to True to any tensor, then PyTorch will automatically track and calculate gradients w.r.t. that tensor.\n",
    "During backpropagation you need gradients of the Loss function wrt to the weights. \n",
    "This is done with the .backward() method, so during this operation tensors with requires_grad set to True will be used along with the tensor used to call backward() with to calculate the gradients.\n",
    "\n",
    "### Example1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "21726644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(12.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(2.0, requires_grad = True)\n",
    "\n",
    "z = x ** 3 # z=x^3 ---> Dz/Dx = 3x**2\n",
    "\n",
    "z.backward()  # Computes the gradient \n",
    "print(x.grad) # this is dz/dx.... You can also try to print it if you comment #z.backward() ---> will return None."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69316dff",
   "metadata": {},
   "source": [
    "Notice that above we used a general function z, but in practice, we use the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0de1257",
   "metadata": {},
   "source": [
    "### Example2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bcc137bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.)\n",
      "tensor(1.)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(5.0)\n",
    "w  = torch.tensor(3.0, requires_grad=True)\n",
    "b = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "y = w*x + b  # y is 3*5 + 2 = 17\n",
    "# Notice that in this example and the one above, y is not a python funciton, but a pytorch tensor!!!\n",
    "y.backward()\n",
    "\n",
    "print(w.grad) # This returns the derivative of y w.r.t w which is equal to x=5\n",
    "print(b.grad) # This returns the derivative of y w.r.t b which is  1\n",
    "print(x.grad) # This will return None as we haven't set requires_grad to True for the tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a058943",
   "metadata": {},
   "source": [
    "https://pytorch.org/docs/stable/generated/torch.Tensor.backward.html#:~:text=Computes%20the%20gradient%20of%20current,function%20additionally%20requires%20specifying%20gradient%20.\n",
    "\n",
    "As far as I understood, Pytorch will create a graph. Possibly through the neural network, where we use many arrays/tensors. For such tensors, when we set the requires_grad=True, when with the output of the graph we use the method .backward()  (p.s. notice that .backward() is a method for any tensor! So the output also is a tensor), it will compute all the derivatives wrt to such tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea9c10e",
   "metadata": {},
   "source": [
    "### One more note: the output, as pytorch tensor, needs to be scalar! In the examples above no problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e765e5",
   "metadata": {},
   "source": [
    "## How To Define A Loss Function in Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "340fae06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Very general example of loss function\n",
    "def my_loss(output, target):\n",
    "    ''' \n",
    "    output: is a tensor, a pytorch tensor. It is the output of the neural network\n",
    "    target: is a tensor, a pytorch tensor. It is facultative argument of the function, when we\n",
    "               have a supervised model. In ODE we won't use it since it is unsupervised, i.e., we don't know\n",
    "               the result function.'''\n",
    "    loss = torch.mean((output - target)**2)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2f766d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_1 = tensor([[ 0.5282, -2.5619, -0.0416],\n",
      "        [-1.0415, -2.1081,  0.1948]], requires_grad=True)\n",
      "target = tensor([[-2.5156,  1.5705,  0.5540],\n",
      "        [-0.6267,  0.7759, -0.7455]])\n",
      "output= tensor(2.0018, grad_fn=<L1LossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Example of code:\n",
    "\n",
    "mae_loss = torch.nn.L1Loss() # Normal arithmetics loss. If you want to use mean square error: .MSELoss\n",
    "\n",
    "input_1 = torch.randn(2, 3, requires_grad=True)\n",
    "print(\"input_1 =\", input_1)\n",
    "\n",
    "target = torch.randn(2, 3)\n",
    "print(\"target =\", target)\n",
    "\n",
    "output = mae_loss(input_1, target)\n",
    "output.backward()\n",
    "print(\"output=\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a40312d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "L1Loss()"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mae_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8cefcc89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1667, -0.1667, -0.1667],\n",
       "        [-0.1667, -0.1667,  0.1667]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_1.grad # Yes, it is the gradient of mean average error f = (x1+x2+x3+x4+x5+x6/n -constants) ---> df/dxi\n",
    "             # Only the signs are not always good, because in the definition they use absolute values | |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07eae94",
   "metadata": {},
   "source": [
    "## more on torch.autograd.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "06413cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 2, 100)[:, None]\n",
    "\n",
    "input_tensor = torch.Tensor(x)\n",
    "input_tensor.requires_grad = True  # Changing its attribute.\n",
    "\n",
    "N = nn.Sequential(nn.Linear(1, 5), nn.Sigmoid(), nn.Linear(5,1, bias=False))\n",
    "Psi_t = lambda x: 3.2 + x * N(x)  # A function, depending on the neural network \n",
    "\n",
    "outputs = Psi_t(input_tensor) \n",
    "\n",
    "output = torch.mean(outputs)   # Scalar output! It is a pytorch tensor, scalar!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "af53fc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output.backward()  # Let's compute the derivatives of output with respect the tensors with requires_grad=True\n",
    "# print(input_tensor.grad) # Derivatives of output wrt to input_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1798f54f",
   "metadata": {},
   "source": [
    "### Notice that if we run again output.backward() gives error. It says to retain_graph=True. What does this mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "0e506acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Psi_t_x = torch.autograd.grad(outputs, input_tensor, grad_outputs=torch.ones_like(outputs),\n",
    "                    create_graph=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a7d1d2cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Psi_t_x)       # 100\n",
    "len(outputs)       # 100\n",
    "len(input_tensor)  # 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd59c88",
   "metadata": {},
   "source": [
    "Left to understand:\n",
    "\n",
    "grad_outputs.\n",
    "\n",
    "ones_like\n",
    "\n",
    "create_graph = True\n",
    "\n",
    "Why in the api it says it returns the sum of the gradients?? \n",
    "https://pytorch.org/docs/stable/generated/torch.autograd.grad.html\n",
    "\n",
    "https://stackoverflow.com/questions/54754153/autograd-grad-for-tensor-in-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9eae084",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62e70c8a",
   "metadata": {},
   "source": [
    "# After the previous, left to understand the optimizer and we're done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfad0167",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697c7e57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0717872",
   "metadata": {},
   "source": [
    "# and also with pytorch.no_grad(): \n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f646e73f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
