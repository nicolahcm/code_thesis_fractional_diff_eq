{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = nn.Sequential(nn.Linear(1, 5), nn.Sigmoid(), nn.Linear(5,1, bias=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How To Recover weights of the neural network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_layer = N[0]\n",
    "second_layer = N[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.9050],\n",
       "        [-0.2882],\n",
       "        [-0.6562],\n",
       "        [ 0.0626],\n",
       "        [ 0.2304]], requires_grad=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_layer.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 0.0095, -0.6441,  0.5135,  0.2547,  0.3641], requires_grad=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_layer.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.1544, -0.1039,  0.2211,  0.2062, -0.4180]], requires_grad=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_layer.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept of autograd and requires_grad = True in Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems that requires_grad = True, and autograd, are needed for recording the history.\n",
    "\n",
    "Not only this, but to compute some derivatives wrt to them. \n",
    "\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html Complete guide.\n",
    "\n",
    "https://jovian.ai/forum/t/what-is-the-use-of-requires-grad-in-tensors/17718 In short, very useful.\n",
    "\n",
    "https://pytorch.org/docs/stable/notes/autograd.html#:~:text=Setting%20requires_grad&text=Parameter%60%60%2C%20that%20allows%20for,its%20input%20tensors%20require%20grad.\n",
    "Hard to understand.\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.Tensor.requires_grad.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you set requires_grad to True to any tensor, then PyTorch will automatically track and calculate gradients w.r.t. that tensor.\n",
    "During backpropagation you need gradients of the Loss function wrt to the weights. \n",
    "This is done with the .backward() method, so during this operation tensors with requires_grad set to True will be used along with the tensor used to call backward() with to calculate the gradients.\n",
    "\n",
    "### Example1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(12.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(2.0, requires_grad = True)\n",
    "\n",
    "z = x ** 3 # z=x^3 ---> Dz/Dx = 3x**2\n",
    "\n",
    "z.backward()  # Computes the gradient \n",
    "print(x.grad) # this is dz/dx.... You can also try to print it if you comment #z.backward() ---> will return None."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that above we used a general function z, but in practice, we use the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.)\n",
      "tensor(1.)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(5.0)\n",
    "w  = torch.tensor(3.0, requires_grad=True)\n",
    "b = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "y = w*x + b  # y is 3*5 + 2 = 17\n",
    "# Notice that in this example and the one above, y is not a python funciton, but a pytorch tensor!!!\n",
    "y.backward()\n",
    "\n",
    "print(w.grad) # This returns the derivative of y w.r.t w which is equal to x=5\n",
    "print(b.grad) # This returns the derivative of y w.r.t b which is  1\n",
    "print(x.grad) # This will return None as we haven't set requires_grad to True for the tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch.org/docs/stable/generated/torch.Tensor.backward.html#:~:text=Computes%20the%20gradient%20of%20current,function%20additionally%20requires%20specifying%20gradient%20.\n",
    "\n",
    "As far as I understood, Pytorch will create a graph. Possibly through the neural network, where we use many arrays/tensors. For such tensors, when we set the requires_grad=True, when with the output of the graph we use the method .backward()  (p.s. notice that .backward() is a method for any tensor! So the output also is a tensor), it will compute all the derivatives wrt to such tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One more note: the output, as pytorch tensor, needs to be scalar! In the examples above no problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How To Define A Loss Function in Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Very general example of loss function\n",
    "def my_loss(output, target):\n",
    "    ''' \n",
    "    output: is a tensor, a pytorch tensor. It is the output of the neural network\n",
    "    target: is a tensor, a pytorch tensor. It is facultative argument of the function, when we\n",
    "               have a supervised model. In ODE we won't use it since it is unsupervised, i.e., we don't know\n",
    "               the result function.'''\n",
    "    loss = torch.mean((output - target)**2)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_1 = tensor([[ 1.3283,  0.1039, -1.0671],\n",
      "        [-1.4755, -0.0457,  0.5895]], requires_grad=True)\n",
      "target = tensor([[-0.7535, -2.0250,  0.4282],\n",
      "        [-0.2317,  0.5642, -0.5844]])\n",
      "output= tensor(1.4556, grad_fn=<L1LossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Example of code:\n",
    "\n",
    "mae_loss = torch.nn.L1Loss() # Normal arithmetics loss. If you want to use mean square error: .MSELoss\n",
    "\n",
    "input_1 = torch.randn(2, 3, requires_grad=True)\n",
    "print(\"input_1 =\", input_1)\n",
    "\n",
    "target = torch.randn(2, 3)\n",
    "print(\"target =\", target)\n",
    "\n",
    "output = mae_loss(input_1, target)\n",
    "output.backward()\n",
    "print(\"output=\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "L1Loss()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mae_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1667,  0.1667, -0.1667],\n",
       "        [-0.1667, -0.1667,  0.1667]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_1.grad # Yes, it is the gradient of mean average error f = (x1+x2+x3+x4+x5+x6/n -constants) ---> df/dxi\n",
    "             # Only the signs are not always good, because in the definition they use absolute values | |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## more on torch.autograd.grad: computing gradients/derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000],\n",
      "        [0.5000],\n",
      "        [1.0000],\n",
      "        [1.5000],\n",
      "        [2.0000]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor(np.linspace(0,2,5)[:,None])\n",
    "x.requires_grad = True\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000],\n",
       "        [0.1250],\n",
       "        [1.0000],\n",
       "        [3.3750],\n",
       "        [8.0000]], grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_x = lambda x: x**3\n",
    "\n",
    "y_x_outputs = y_x(x)\n",
    "y_x_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_der_x = torch.autograd.grad(y_x_outputs, x)   GIVES ERROR, scalar error, so we also use\n",
    "#                                                 grad_outputs = torch.ones_like(x)\n",
    "\n",
    "# https://stackoverflow.com/questions/54754153/autograd-grad-for-tensor-in-pytorch#\n",
    "# for more info see the link above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0000],\n",
       "         [ 0.7500],\n",
       "         [ 3.0000],\n",
       "         [ 6.7500],\n",
       "         [12.0000]]),)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_der_x = torch.autograd.grad(outputs=y_x_outputs, inputs=x, grad_outputs=torch.ones_like(y_x_outputs))\n",
    "\n",
    "# This is very similar to calling output.backward()\n",
    "# But with 2 differences:\n",
    "# 1) First .backward() requires to be applied to a scalar. Therefore we took output = mean(outputs)\n",
    "# 2) We use this and not backward, because this is an intermediate step. The final backward is used\n",
    "#    for the neural network and the weights adjusting. These derivatives in here are only an intermediate step\n",
    "#    , more precisely, in this case, only to compute the Loss Function\n",
    "\n",
    "y_der_x    #is a tuple with one element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000],\n",
       "        [ 0.7500],\n",
       "        [ 3.0000],\n",
       "        [ 6.7500],\n",
       "        [12.0000]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_der_x[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 2, 100)[:, None]\n",
    "\n",
    "input_tensor = torch.Tensor(x)\n",
    "input_tensor.requires_grad = True  # Changing its attribute.\n",
    "\n",
    "N = nn.Sequential(nn.Linear(1, 5), nn.Sigmoid(), nn.Linear(5,1, bias=False))\n",
    "Psi_t = lambda x: 3.2 + x * N(x)  # A function, depending on the neural network \n",
    "\n",
    "outputs = Psi_t(input_tensor) \n",
    "\n",
    "output = torch.mean(outputs)   # Scalar output! It is a pytorch tensor, scalar!\n",
    "                               # Required to make output scalar, because .backward() applies only to a scalar.\n",
    "                               # That is, outputs.backward() gives error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output.backward()  # Let's compute the derivatives of output with respect the tensors with requires_grad=True\n",
    "# print(input_tensor.grad) # Derivatives of output wrt to input_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that if we run again output.backward() gives error. It says to retain_graph=True. What does this mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Psi_t_x = torch.autograd.grad(outputs, input_tensor, grad_outputs=torch.ones_like(outputs),\n",
    "                    create_graph=True)[0] \n",
    "\n",
    "# create_graph (bool, optional) â€“ If True, graph of the derivative will be constructed, \n",
    "# allowing to compute higher order derivative products. Default: False.\n",
    "# Especially we need it when we want to compute second derivatives etc.. etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Psi_t_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After the previous, left to understand the optimizer and we're done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# and also with pytorch.no_grad(): \n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
